% This file was created with JabRef 2.6.
% Encoding: UTF8

@INPROCEEDINGS{scharl2012,
    author = {Arno Scharl and Marta Sabou and Stefan Gindl and Walter Rafelsberger and Albert Weichselbraun},
    title = {Leveraging the Wisdom of the Crowds for the Acquisition of Multilingual Language Resources},
    booktitle = {8th International Conference on Language Resources and Evaluation (LREC-2012)},
    address = {Istambul, Tureky},
    year={2012},
    note = {Forthcoming (accepted 1 February 2012)},
    eprint = {http://eprints.weblyzard.com/35/1/scharl2012-leveragingTheWisdomOfCrowds.pdf},
    keywords = {crowdsourcing, language resource acquisition, sentiment detection, sentiment analysis, semantic orientation},
    abstract = {Games with a purpose are an increasingly popular mechanism for leveraging the wisdom of the crowds to address tasks which are trivial for humans but still not solvable by computer algorithms in a satisfying manner. As a novel mechanism for structuring human-computer interactions, a key challenge when creating them is motivating users to participate while generating useful and unbiased results. This paper focuses on important design choices and success factors of effective games with a purpose. Our findings are based on lessons learned while developing and deploying Sentiment Quiz, a crowdsourcing application for creating sentiment lexicons (an essential component of most sentiment detection algorithms). We describe the goals and structure of the game, the underlying application framework, the sentiment lexicons gathered through crowdsourcing, as well as a novel approach to automatically extend the lexicons by means of a bootstrapping process. Such an automated extension further increases the efficiency of the acquisition process by limiting the number of terms that need to be gathered from the game participants.}
}

@INPROCEEDINGS{lang2012,
  author = {Heinz-Peter Lang and Gerhard Wohlgenannt and Albert Weichselbraun},
  title = {TextSweeper - A System for Content Extraction and Overview Page Detection},
  booktitle = {International Conference on Information Resources Management (Conf-IRM)},
  year = {2012},
  address = {Vienna, Austria},
  publisher = {AIS},
  note = {Forthcoming (accepted 6 February 2012)},
  owner = {albert},
  timestamp = {2012.02.06},
  keywords = {content extraction, overview pages, text filtering, natural language processing, contextualized information spaces, Web-based information retrieval},
  abstract = {Web pages not only contain main content, but also other elements such as navigation panels, advertisements and links to related documents.
  Furthermore, overview pages (summarization pages and entry points) duplicate and aggregate parts of articles and thereby create redundancies. The noise elements in Web pages as well as overview pages affect the performance of downstream processes such as Web-based Information Retrieval. Context Extraction's task is identifying and extracting the main content from a Web page.
 In this research-in-progress paper we present an approach which not only identifies and extracts the main content,
  but also detects overview pages and thereby allows skipping them. The content extraction part of the system is an extension of existing Text-to-Tag ratio methods, overview page detection is accomplished with the net text length heuristic. Preliminary results and ad-hoc evaluation indicate a promising system performance. A formal evaluation and comparison to other state-of-the-art approaches is part of future work.}
}

@INPROCEEDINGS{brown2006,
  author = {Alistair Brown and Stace, Roger and Purushothaman, Maya and Scharl,
	Arno and Weichselbraun, Albert},
  title = {Accounting for Pacific Island Countries' Tourism},
  booktitle = {British Accounting Association Annual Conference (BAA-2006)},
  year = {2006},
  address = {Portsmouth, UK},
  publisher = {British Accounting Association},
  owner = {albert},
  timestamp = {2008.07.15}
}

@INCOLLECTION{dickinger2008,
  author = {Dickinger, Astrid and Scharl, Arno and Stern, Hermann and Weichselbraun,
	Albert and Wöber, Karl},
  title = {Acquisition and Relevance of Geotagged Information in Tourism},
  booktitle = {Information and Communication Technologies in Tourism 2008, Proceedings
	of the International Conference in Innsbruck, Austria, 2008},
  publisher = {Springer},
  year = {2008},
  editor = {O'Connor, Peter and Höpken Wolfram and Gretzel Ulrike},
  pages = {545--555},
  address = {Vienna-New York},
  abstract = {In the case of tourism applications, it is particularly evident that
	geography is emerging as a fundamental principle for structuring
	Web resources. Recent improvements in semantic and geographic Web
	technology, often referred to as the Geospatial Web, acknowledge
	the relevance of adding location metadata to existing databases and
	accessing the vast amounts of information stored in these databases
	via geospatial services. This paper outlines the acquisition of geospatial
	context information, describes usage scenarios and real-world applications
	in the tourism industry, and presents an automated software tool
	for annotating large collections of Web documents automatically.
	The quality of this tool is tested based upon Web pages from the
	Austrian National Tourism Organization. Initial results are encouraging
	and help define a roadmap for further improving the automated tagging
	of tourism resources.},
  doi = {10.1007/978-3-211-77280-5_48}
}

@INPROCEEDINGS{dickinger2005,
  author = {Dickinger, Astrid and Scharl, Arno and Weichselbraun, Albert},
  title = {Where Do You Want to Go Today? A Media Analysis of Global Tourism
	Destinations},
  booktitle = {Proceedings of the 5th International Conference on Knowledge Management
	(I-KNOW 2005)},
  year = {2005},
  editor = {Tochtermann, Klaus and Maurer, Hermann},
  pages = {20--27},
  address = {Graz, Austria},
  owner = {albert},
  timestamp = {2008.07.15}
}

@INCOLLECTION{gindl2009,
  author = {Gindl, Stefan and Liegl, Johannes and Scharl, Arno and Weichselbraun, Albert},
  title = {An Evaluation Framework and Adaptive Architecture for Automated Sentiment Detection},
  booktitle = {Networked Knowledge - Networked Media: Integrating Knowledge Management, New Media Technologies and Semantic Systems},
  publisher = {Springer},
  year = {2009},
  editor = {Pellegrini, T. and Auer, S. and Tochtermann, K. and Schaffert, S.},
  volume = {221},
  series = {Studies in Computational Intelligence},
  pages = {217-234},
  address = {Heidelberg},
  owner = {albert},
  timestamp = {2009.10.30}
}

@INPROCEEDINGS{gindl2010,
  author = {Gindl, Stefan and Scharl, Arno and Weichselbraun, Albert},
  title = {Generic High-Throughput Methods for Multilingual Sentiment Detection},
  booktitle = {4th IEEE International Conference on Digital Ecosystems and Technologies},
  year = {2010},
  editor = {F. K. Hussain and E. Chang},
  pages = {239-244},
  address = {Dubai, United Arab Emirates},
  publisher = {IEEE Computer Society Press},
  abstract = {Digital ecosystems typically involve a large number of participants
	from different sectors who generate rapidly growing archives of unstructured
	text. Measuring the frequency of certain terms to determine the popularity
	of a topic is comparably straightforward. Detecting sentiment expressed
	in user-generated electronic content is more challenging, especially
	in the case of digital ecosystems comprising heterogeneous sets of
	multilingual documents. This paper describes the use of language-specific
	grammar patterns and multilingual tagged dictionaries to detect sentiment
	in German and English document repositories. Digital ecosystems may
	contain millions of frequently updated documents, requiring sentiment
	detection methods that maximize throughput. The ideal combination
	of high-throughput techniques and more accurate (but slower) approaches
	depends on the specific requirements of an application. To accommodate
	a wide range of possible applications, this paper presents (i) an
	adaptive method, balancing accuracy and scalability of multilingual
	textual sources, (ii) a generic approach for generating language-specific
	grammar patterns and multilingual tagged dictionaries, and (iii)
	an extensive evaluation verifying the method's performance based
	on Amazon product reviews and user evaluations from Sentiment Quiz,
	a game-with-a-purpose that invites users of the Facebook social networking
	platform to assess the sentiment of individual sentences. },
  eprint = {http://eprints.weblyzard.com/20/1/dest-2010-submission.pdf},
  owner = {albert},
  timestamp = {2010.04.14}
}

@INPROCEEDINGS{gindl2010a,
  author = {Gindl, Stefan and Weichselbraun, Albert and Scharl, Arno},
  title = {Cross-Domain Contextualisation of Sentiment Lexicons},
  booktitle = {19th European Conference on Artificial Intelligence (ECAI)},
  year = {2010},
  editor = {Helder Coelho and Rudi Studer and Michael Wooldridge},
  volume = {215},
  series = {Frontiers in Artificial Intelligence and Applications},
  pages = {771--776},
  address = {Lisbon, Portugal},
  month = {August},
  publisher = {IOS Press},
  abstract = {The simplicity of using Web 2.0 platforms and services has resulted
	in an abundance of user-generated content. A significant part of
	this content contains user opinions with clear economic relevance
	- customer and travel reviews, for example, or the articles of well-known
	and respected bloggers who influence purchase decisions. Analyzing
	and acting upon user-generated content is becoming imperative for
	marketers and social scientists who aim to gather feedback from very
	large user communities. Sentiment detection, as part of opinion mining,
	supports these efforts by identifying and aggregating polar opinions
	- i.e., positive or negative statements about facts. For achieving
	accurate results, sentiment detection requires a correct interpretation
	of language, which remains a challenging task due to the inherent
	ambiguities of human languages. Particular attention has to be directed
	to the context of opinionated terms when trying to resolve these
	ambiguities. Contextualized sentiment lexicons address this need
	by considering the sentiment term's context in their evaluation but
	are usually limited to one domain, as many contextualizations are
	not stable across domains. This paper introduces a method which identifies
	unstable contextualizations and refines the contextualized sentiment
	dictionaries accordingly, eliminating the need for specific training
	data for each individual domain. An extensive evaluation compares
	the accuracy of this approach with results obtained from domain-specific
	corpora. },
  doi = {http://dx.doi.org/10.3233/978-1-60750-606-5-771},
  eprint = {http://eprints.weblyzard.com/19/1/ECAI%2D328.pdf},
  file = {gindl2010a-crossDomainContextualization.pdf:literature/self/gindl2010a-crossDomainContextualization.pdf:PDF},
  isbn = {978-1-60750-605-8},
  owner = {albert}
}

@INCOLLECTION{granitzer2007,
  author = {Granitzer, Michael and Scharl, Arno and Weichselbraun, Albert and
	Neidhart, Thomas and Juffinger, Andreas and Wohlgenannt, Gerhard},
  title = {Automated Ontology Learning and Validation Using Hypothesis Testing},
  booktitle = {Advances in Intelligent Web Mastering},
  publisher = {Springer},
  year = {2007},
  editor = {Wegrzyn-wolska, Katarzyna M. and Szczepaniak, Piotr S.},
  number = {43/2007},
  series = {Advances in Soft Computing},
  pages = {130--135},
  address = {Berlin-Heidelberg},
  abstract = {Semantic Web technologies in general and ontology-based approaches
	in particular are considered the foundation for the next generation
	of information services. While ontologies enable software agents
	to exchange knowledge and information in a standardized, intelligent
	manner, describing today's vast amount of information in terms of
	ontological knowledge remains a challenge. In this paper we describe
	the research project AVALON - Acquisition and VALidation of ONtologies,
	which aims at reducing the knowledge acquisition bottleneck by using
	methods from ontology learning in the context of a cybernetic control
	system. We will present techniques allowing us to automatically extract
	knowledge from textual data and formulating hypothesis based upon
	the extracted knowledge. Based on real world indicators, like for
	example business numbers, hypotheses are validated and the result
	is fed back into the system, thereby closing the cybernetic control
	system¿s feedback loop. While AVALON is currently under development,
	we will present intermediate results and the basic idea behind the
	system.},
  doi = {10.1007/978-3-540-72575-6_21},
  isbn = {978-3-540-72574-9},
  issn = {1615-3871},
  owner = {albert},
  timestamp = {2008.07.14}
}

@INPROCEEDINGS{scharl2008e,
  author = {Hubmann-Haidvogel, Alexander and Scharl, Arno and Weichselbraun,
	Albert},
  title = {Tightly Coupled Views for Navigating Content Repositories},
  booktitle = {14th Brazilian Symposium on Multimedia and the Web (WebMedia-2008)},
  year = {2008},
  editor = {J. G. Pereira Filho and R. L. Gomes and M. A. Gerosa and R. S. S.
	Guizzardi},
  pages = {5--8},
  address = {Vila Velha, Brazil},
  publisher = {Brazilian Computing Society},
  abstract = { The advantages and positive effects of tightly coupled interface
	components on search performance have been documented in several
	studies. This paper focuses on the implementation of tightly coupled
	views within the Media Watch on Climate Change, an interactive Web
	portal (www.ecoresearch.net/climate) combining a portfolio of semantic
	services with a visual exploration and information retrieval interface.
	The portal builds contextualized information spaces by (a) enriching
	the content repository with spatial, semantic, and temporal annotations,
	and (b) applying semi-automated ontology learning to the repository
	yielding a controlled vocabulary that helps structuring the stored
	information. Different portlets visualize aspects of the contextualized
	information spaces, providing the user with multiple views on the
	available information. Currently, synchronized semantic maps, domain
	ontologies, geographic maps, tag clouds and real-time information
	retrieval agents suggesting similar topics and nearby locations,
	provide users with important context information, facilitate the
	access to complex datasets, and help users navigate large collections
	of Web documents. },
  owner = {albert},
  timestamp = {2008.10.27}
}

@ARTICLE{hubmann2009,
  author = {Hubmann-Haidvogel, Alexander and Scharl, Arno and Weichselbraun,
	Albert},
  title = {Multiple Coordinated Views for Searching and Navigating Web Content
	Repositories},
  journal = {Information Sciences},
  year = {2009},
  volume = {179},
  pages = {1813--1821},
  number = {12},
  abstract = { The advantages and positive effects of multiple coordinated views
	on search performance have been documented in several studies. This
	paper describes the implementation of multiple coordinated views
	within the Media Watch on Climate Change, a domain-specific news
	aggregation portal available at www.ecoresearch.net/climate that
	combines a portfolio of semantic services with a visual information
	exploration and retrieval interface. The system builds contextualized
	information spaces by enriching the content repository with geospatial,
	semantic and temporal annotations, and by applying semi-automated
	ontology learning to create a controlled vocabulary for structuring
	the stored information. Portlets visualize the different dimensions
	of the contextualized information spaces, providing the user with
	multiple views on the latest news media coverage. Context information
	facilitates access to complex datasets and helps users navigate large
	repositories of Web documents. Currently, the system synchronizes
	information landscapes, domain ontologies, geographic maps, tag clouds
	and just-in-time information retrieval agents that suggest similar
	topics and nearby locations. },
  date = {30 May 2009},
  doi = {10.1016/j.ins.2009.01.030},
  eprint = {http://eprints.weblyzard.com/8/1/infoscience.pdf},
  file = {hubmann2009-contentRepositories.pdf:literature/self/hubmann2009-contentRepositories.pdf:PDF},
  owner = {albert},
  timestamp = {2009.03.02}
}

@ARTICLE{juffinger2009,
  author = {Andreas Juffinger and Thomas Neidhart and Michael Granitzer and Roman
	Kern and Albert Weichselbraun and Gerhard Wohlgenannt and Arno Scharl},
  title = {Distributed Web2.0 Crawling for Ontology Evolution},
  journal = {Journal of Digital Information Management},
  year = {2009},
  volume = {7},
  pages = {114-119},
  number = {2},
  date = {April 2009},
  owner = {albert},
  timestamp = {2009.03.02}
}

@INPROCEEDINGS{juffinger2007,
  author = {Juffinger, Andreas and Neidhart, Thomas and Granitzer, Michael and
	Kern, Roman and Weichselbraun, Albert and Wohlgenannt, Gerhard and
	Scharl, Arno},
  title = {Distributed Web2.0 Crawling for Ontology Evolution},
  booktitle = {Proceedings of the Second International Conference on Digital Information
	Management (ICDIM'07)},
  year = {2007},
  pages = {615--620},
  address = {Lyon, France},
  doi = {10.1109/ICDIM.2007.4444293},
  month = {October},
  abstract = {Semantic Web technologies in general and ontology-
	
	based approaches in particular are considered the foundation 
	
	for the next generation of information services. While
	
	ontologies enable software agents to exchange knowledge
	
	and information in a standardised, intelligent manner, describing
	
	
	todays vast amount of information in terms of ontological 
	
	knowledge and to track the evolution of such ontologies 
	
	remains a challenge.
	
	 In this paper we describe Web2.0 crawling for ontology
	
	evolution. The World Wide Web, or Web for short, is due
	
	its evolutionary properties and social network characteristics 
	
	a perfect fitting data source to evolve an ontology. The
	
	decentralised structure of the Internet, the huge amount of
	
	data and upcoming Web2.0 technologies arise several challenges 
	
	for a crawling system. In this paper we present a
	
	distributed crawling system with standard browser integration.
	
	 The proposed system is a high performance, sitescript
	
	based noise reducing crawler which loads standard browser
	
	equivalent content from Web2.0 resources. Furthermore we
	
	describe the integration of this spider into our ontology evolution
	framework.},
  owner = {albert},
  timestamp = {2007.10.31}
}

@ARTICLE{liu2005,
  author = {Liu, Wei and Weichselbraun, Albert and Scharl, Arno and Chang, Elizabeth},
  title = {Semi-Automatic Ontology Extension Using Spreading Activation},
  journal = {Journal of Universal Knowledge Management},
  year = {2005},
  volume = {0},
  pages = {50--58},
  number = {1},
  abstract = {This paper describes a system to semi-automatically extend and refine
	ontologies by mining textual data from the Web sites of international
	online media. Expanding a seed ontology creates a semantic network
	through co-occurrence analysis, trigger phrase analysis, and disambiguation
	based on the WordNet lexical dictionary. Spreading activation then
	processes this semantic network to find the most probable candidates
	for inclusion in an extended ontology. Approaches to identifying
	hierarchical relationships such as subsumption, head noun analysis
	and WordNet consultation are used to confirm and classify the found
	relationships. Using a seed ontology on climate change as an example,
	this paper demonstrates how spreading activation improves the result
	by naturally integrating the mentioned methods.},
  eprint = {http://eprints.weblyzard.com/4/1/wei-ontologyExtension2005.pdf},
  file = {liu2005-ontologyExtension.pdf:literature/self/liu2005-ontologyExtension.pdf:PDF},
}

@UNPUBLISHED{weichselbraun2005t,
  author = {Liu, Wei and Weichselbraun, Albert and Scharl, Arno and Chang, Elizabeth},
  title = {Semi-Automatic Ontology Extension Using Spreading Activation},
  note = {International Conference on Knowledge Management (iKnow 2005), Graz,
	Austria},
  month = {30 June},
  year = {2005},
  eprint = {http://eprints.weblyzard.com/1/1/i-know2005.pdf},
  owner = {albert},
  timestamp = {2008.07.15}
}

@ARTICLE{pollach2009,
  author = {Pollach, Irene and Scharl, Arno and Weichselbraun, Albert},
  title = {Web content mining for comparing corporate and third party online
	reporting: a case study on solid waste management},
  journal = {Business Strategy and the Environment},
  year = {2009},
  volume = {18},
  pages = {137-148},
  number = {3},
  abstract = { This study investigates the coverage of solid waste management on
	1142 websites maintained by companies, news media and non-governmental
	organizations to validate an automated approach to content and language
	analysis. First, a frequency analysis of waste management terms sheds
	light on the breadth and depth of their environmental discourses,
	revealing that corporate and media attention to waste management
	is small compared with that of non-governmental organizations. Second,
	an investigation of their attitudes toward waste management suggests
	that companies avoid negative information in environmental communication,
	unlike news media or non-governmental organizations. Ultimately,
	an automated tool for ontology building is employed to gain insights
	into companies' shared understanding of waste management. The ontology
	obtained indicates that companies conceptualize waste management
	as a business process rather than framing it from an ecological perspective,
	which is in line with findings from previous research.},
  eprint = {http://eprints.weblyzard.com/3/1/BSEsubmission_revised.pdf},
  file = {pollach2009-wasteManagement.pdf:literature/self/pollach2009-wasteManagement.pdf:PDF},
  owner = {albert},
  timestamp = {2008.07.15},
  url = {http://www3.interscience.wiley.com/journal/112728524/abstract}
}

@ARTICLE{scharl2008,
  author = {Scharl, Arno and Dickinger, Astrid and Weichselbraun, Albert},
  title = {Analyzing News Media Coverage to Acquire and Structure Tourism Knowledge},
  journal = {Information Technology and Tourism},
  year = {2008},
  volume = {10},
  pages = {3--17},
  number = {1},
  abstract = {Destination image significantly influences a tourist's decision-making
	process. The impact of news media coverage
	
	on destination image has attracted research attention and became particularly
	evident after catastrophic events such
	
	as the 2004 Indian Ocean earthquake that triggered a series of lethal
	tsunamis. Building upon previous research, this
	
	paper analyzes the prevalence of tourism destinations among 162 international
	media sites. Term frequency captures
	
	the attention a destination receives ­ from a general and, after contextual
	filtering, from a tourism perspective. Calculating 
	
	sentiment estimates positive and negative media influences on destination
	image at a given point in time.
	
	Identifying semantic associations with the names of countries and
	major cities, the results of co-occurrence analysis
	
	reveal the public profiles of destinations, and the impact of current
	events on media coverage. These results allow
	
	national tourism organizations to assess how their destination is
	covered by news media in general, and in a specific
	
	tourism context. To guide analysts and marketers in this assessment,
	an iterative analysis of semantic associations
	
	through natural language processing extracts tourism knowledge automatically,
	and represents this knowledge as
	
	ontological structures.},
  eprint = {http://eprints.weblyzard.com/15/1/scharl2008-MediaCoverage.pdf},
  file = {scharl2008-tourismKnowledge.pdf:literature/self/scharl2008-tourismKnowledge.pdf:PDF},
  owner = {albert},
  timestamp = {2008.01.27}
}

@INPROCEEDINGS{scharl2008a,
  author = {Scharl, Arno and Stern, Hermann and Weichselbraun, Albert},
  title = {Annotating and Visualizing Location Data in Geospatial Web Applications},
  booktitle = {Proceedings of the First International Workshop on Location and the
	Web (LocWeb 2008)},
  year = {2008},
  editor = {Boll, Susanne and Wilde, Erik},
  address = {Beijing, China},
  month = {April},
  abstract = {This paper presents the IDIOM Media Watch on Climate Change (www.ecoresearch.net/climate),
	a prototypical implementation of an environmental portal that emphasizes
	the importance of location data for advanced Web applications. The
	introductory section outlines the process of retrofitting existing
	knowledge repositories with geographical context information, a process
	also referred to as geotagging. The paper then describes the portal's
	functionality, which aggregates, annotates and visualizes environmental
	articles from 150 Anglo-American news media sites. From 300,000 news
	media articles gathered in weekly intervals, the system selects about
	10,000 focusing on environmental issues. The crawled data is indexed
	and stored in a central repository. Geographic location represents
	a central aspect of the application, but not the only dimension used
	to organize and filter content. Applying the concepts of location
	and topography to semantic similarity, the paper concludes with discussing
	information landscapes as alternative interface metaphor for accessing
	large Web repositories.},
  isbn = {978-1-60558-160-6},
  owner = {albert},
  timestamp = {2008.04.23},
  url = {http://medien.informatik.uni-oldenburg.de/LocWeb2008/}
}

@INPROCEEDINGS{scharl2008f,
  author = {Arno Scharl and Hermann Stern and Albert Weichselbraun},
  title = {A Geospatial Web Application for Communicating Climate Change},
  booktitle = {11th International Conference on Geographic Information Science (AGILE-2008),
	GeoVisualization of Dynamics, Movement and Change Workshop, AGILE
	Council},
  year = {2008},
  address = {Girona, Spain},
  month = {May},
  owner = {albert},
  
}

@INCOLLECTION{scharl2010,
  author = {Scharl, Arno and Weichselbraun, Albert},
  title = {Building a Web-Based Knowledge Repository on Climate Change to Support
	Environmental Communities},
  booktitle = {Organizational, Business, and Technological Aspects of the Knowledge
	Society: 3rd World Summit on the Knowledge Society (WSKS-2010), Proceedings
	Part II},
  publisher = {Springer},
  year = {2010},
  editor = {Lytras, Miltiadis D. and Pablos, Patricia Ordonez de and Ziderman,
	Adrian and Roulstone, Alan and Maurer, Hermann and Imber, Jonathan
	B.},
  volume = {112},
  series = {Communications in Computer and Information Science},
  pages = {79--84},
  address = {Heidelberg},
  abstract = {This paper presents the technology base and roadmap of the Climate
	Change Collaboratory, a Web-based platform that aims to strengthen
	the relations between scientists, educators, environmental NGOs,
	policy makers, news media and corporations - stakeholders who recognize
	the need for adaptation and mitigation, but differ in world-views,
	goals and agendas. The collaboratory manages expert knowledge and
	provides a platform for effective communication and collaboration.
	It aims to assist networking with leading international organizations,
	bridges the science-policy gap and promotes rich, self-sustaining
	community interaction to translate knowledge into coordinated action.
	Innovative survey instruments in the tradition of "games with a purpose"
	will create shared meaning through collaborative ontology building
	and leverage social networking platforms to capture indicators of
	environmental attitudes, lifestyles and behaviors.},
  doi = {10.1007/978-3-642-16324-1_9},
  eprint = {http://eprints.weblyzard.com/26/1/scharl2010%2DknowledgeRepository.pdf}
}

@UNPUBLISHED{weichselbraun2011ta,
  author = {Arno Scharl and Albert Weichselbraun},
  title = {Context Aware Sentiment Detection},
  note = {Curtin University, Digital Ecosystems and Business Intelligence Institute,
	Perth, Australia},
  month = {10 February},
  year = {2011},
  eprint = {http://eprints.weblyzard.com/28/1/curtin2011.pdf},
  owner = {albert},
  url = {http://www.debii.curtin.edu.au/research/seminars/385-debii-seminar-by-prof-arno-scharl-and-professor-albert-weichselbraun-on-context-aware-sentiment-detection-and-and-mr-saeed-danesh-on-subsymbolic-sentiment-detection-using-artificial-neural-networks.html}
}

@ARTICLE{scharl2008b,
  author = {Scharl, Arno and Weichselbraun, Albert},
  title = {An Automated Approach to Investigating the Online Media Coverage
	of {US} Presidential Elections},
  journal = {Journal of Information Technology \& Politics},
  year = {2008},
  volume = {5},
  pages = {121-132},
  number = {1},
  abstract = {This paper presents the U.S. Election 2004 Web Monitor, a public Web
	portal that 
	
	captured trends in political media coverage before and after the 2004
	U.S. presidential election. 
	
	Developed by the authors of this article, the webLyzard suite of Web
	mining tools provided the required
	
	functionality to aggregate and analyze about a half-million documents
	in weekly intervals. The study
	
	paid particular attention to the editorial slant, which is defined
	as the quantity and tone of a Web site's
	
	coverage as influenced by its editorial position. The observable attention
	and attitude toward the candidates 
	
	served as proxies of editorial slant. The system identified attention
	by determining the frequency 
	
	of candidate references and measured attitude towards the candidate
	by looking for positive and
	
	negative expressions that co-occur with these references. Keywords
	and perceptual maps summarized the
	
	most important topics associated with the candidates, placing special
	emphasis on environmental issues.},
  eprint = {http://eprints.weblyzard.com/7/1/jitp-2008.pdf},
  file = {scharl2008b-presidentialElections.pdf:literature/self/scharl2008b-presidentialElections.pdf:PDF},
  owner = {albert},
  timestamp = {2008.07.10},
  url = {http://www.jitp.net/m_archive.php?p=5}
}

@INPROCEEDINGS{scharl2006,
  author = {Scharl, Arno and Weichselbraun, Albert},
  title = {Web Coverage of the 2004 {US} Presidential Election},
  booktitle = {11th Conference of the European Chapter of the Association for Computational
	Linguistics (EACL-2006)},
  year = {2006},
  pages = {35-42},
  address = {Trento, Italy},
  publisher = {Association for Computational Linguistics},
  owner = {albert},
  timestamp = {2008.01.27}
}

@INPROCEEDINGS{scharl2008c,
  author = {Scharl, Arno and Weichselbraun, Albert and Gindl, Stefan},
  title = {Building Tagged Linguistic Unit Databases for Sentiment Detection},
  booktitle = {Proceedings of the 8th International Conference on Knowledge Management
	(I-Know '08)},
  year = {2008},
  editor = {Tochtermann, Klaus and Maurer, Hermann},
  address = {Graz, Austria},
  month = {September},
  abstract = {Despite the obvious business value of visualizing similarities between
	elements of evolving information spaces and mapping these similarities
	e.g. onto geospatial reference systems, analysts are often more interested
	in how the semantic orientation (sentiment) towards an organization,
	a product or a particular technology is changing over time. Unfortunately,
	popular methods that process unstructured textual material to detect
	semantic orientation automatically based on tagged dictionaries are
	not capable of fulfilling this task, even when coupled with part-of-speech
	tagging, a standard component of most text processing toolkits that
	distinguishes grammatical categories such as article (AT), noun (NN),
	verb (VB), and adverb (RB). Small corpus size, ambiguity and subtle
	incremental change of tonal expressions between different versions
	of a document complicate the detection of semantic orientation and
	often prevent promising algorithms from being incorporated into commercial
	applications. Parsing grammatical structures, by contrast, outperforms
	dictionary-based approaches in terms of reliability, but usually
	suffers from poor scalability due to their computational complexity.
	This paper addresses this predicament by presenting an alternative
	approach based on automatically building Tagged Linguistic Unit (TLU)
	databases to overcome the restrictions of dictionaries with a limited
	set of tagged tokens.},
  owner = {albert}
}

@ARTICLE{scharl2007a,
  author = {Scharl, Arno and Weichselbraun, Albert and Liu, Wei},
  title = {Tracking and modelling information diffusion across interactive online
	media},
  journal = {International Journal of Metadata, Semantics and Ontologies},
  year = {2007},
  volume = {2},
  pages = {136--145},
  number = {2},
  abstract = {Information spreads rapidly across Web sites, Web logs and online
	forums. This
	
	paper describes the research framework of the IDIOM Project (Information
	Diffusion
	
	across Interactive Online Media),1 which analyzes this process by
	identifying redundant
	
	content elements, mapping them to an ontological knowledge structure,
	and tracking their
	
	temporal and geographic distribution. Linguists define "idiom" as
	an expression whose
	
	meaning is different from the literal meanings of its component words.
	Similarly, the study
	
	of information diffusion promises insights that cannot be inferred
	from individual network
	
	elements. This paper presents underlying technology, initial results,
	and the future roadmap
	
	of investigating information diffusion based on ontological knowledge
	structures. Similar
	
	projects often focus on particular media, or neglect important aspects
	of the human language. 
	
	This paper addresses these gaps to reveal fundamental mechanisms of
	information
	
	diffusion across media with distinct interactive characteristics.},
  doi = {10.1504/IJMSO.2007.016807},
  eprint = {http://eprints.weblyzard.com/5/1/scharl-informationDiffusion2007.pdf},
  file = {scharl2007a-informationDiffusion.pdf:literature/self/scharl2007a-informationDiffusion.pdf:PDF},
  owner = {albert},
  timestamp = {2008.03.19},
  url = {http://www.inderscience.com/search/index.php?action=record\&rec_id=16807\&prevQuery=\&ps=10\&m=or}
}

@INPROCEEDINGS{scharl2006a,
  author = {Scharl, Arno and Weichselbraun, Albert and Liu, Wei},
  title = {An Ontology-Based Architecture for Tracking Information across Interactive
	Electronic Environments},
  booktitle = {Proceedings of the 39th Hawaii International Conference on System
	Sciences (HICSS-39)},
  year = {2006},
  pages = {160b},
  doi = {10.1109/HICSS.2006.60},
  address = {Kauai, Hawaii},
  month = {January},
  publisher = {IEEE Computer Society Press},
  owner = {albert},
  timestamp = {2007.10.31}
}

@INPROCEEDINGS{scharl2008d,
  author = {Scharl, Arno and Weichselbraun, Albert and Wohlgenannt, Gerhard},
  title = {A Web-based User Interaction Framework for Collaboratively Building
	and Validating Ontologies},
  booktitle = {8th Brazilian Symposium on Human Factors in Computer Systems},
  year = {2008},
  address = {Porto Alegre, Brazil},
  publisher = {Brazilian Computing Society},
  abstract = { Collaborative ontology building requires innovative navigational
	instruments that improve content exploration and the creation of
	shared meaning. Building upon an existing architecture for automated
	ontology learning from unstructured textual resources developed by
	the authors, this paper presents a Web-based user interaction framework
	encompassing three major components: (i) real-time visualizations
	of ontology evolution with time interval and confidence sliders,
	(ii) traditional ontology editing environments, and (iii) multi-player
	online games leveraging social networking platforms in the tradition
	of games with a purpose. A prototype in the environmental domain
	will showcase the integration of powerful search capabilities with
	novel graph-based interfaces for guiding novice and expert users
	alike. },
  pages = {244--247},
  isbn = {978-85-7669-203-4}
}

@ARTICLE{stace2007,
  author = {Stace, Roger and Brown, Alistair and Purushothaman, Maya and Scharl,
	Arno and Weichselbraun, Albert},
  title = {National Indicators of Well Being: Lessons from Pacific Island Countries},
  journal = {Asia Pacific Journal of Tourism Research},
  year = {2007},
  volume = {12},
  pages = {203--223},
  number = {3},
  file = {:literature/self/stace2007-indicatorsOfWellBeing.pdf:PDF},
  owner = {albert},
  timestamp = {2008.07.15}
}

@INPROCEEDINGS{walter2003,
  author = {Walter, Heimo and Weichselbraun, Albert},
  title = {Comparison of four Finite-Volume-Algorithms for the dynamic simulation
	of natural circulation steam generators},
  booktitle = {ARGESIM Report No 24 Vol II, Hrsg. ARGESIM},
  year = {2003},
  pages = {531-540},
  address = {Vienna, Austria},
  organization = {Vienna University of Technology},
  owner = {albert},
  timestamp = {2008.07.15}
}

@BOOK{walter2002,
  title = {Ein Vergleich unterschiedlicher Finite-Volumen-Verfahren zur dynamischen
	Simulation beheizter Rohrnetzwerke},
  publisher = {VDI-Verlag},
  year = {2002},
  author = {Walter, Heimo and Weichselbraun, Albert},
  address = {Düsseldorf},
  isbn = {3-18-347706-8},
  owner = {albert},
  timestamp = {2008.07.14}
}

@INPROCEEDINGS{weichselbraun2008,
  author = {Weichselbraun, Albert},
  title = {Strategies for Optimizing Querying Third Party Resources in Semantic
	Web Applications},
  booktitle = {3rd International Conference on Software and Data Technologies (ICSOFT-2008)},
  year = {2008},
  editor = {Cordeiro, Jose and Shishkov, Boris and Ranchordas, AlpeshKumar and
	Helfer, Markus},
  pages = {111--118},
  address = {Porto, Portugal},
  month = {July},
  abstract = {One key property of the Semantic Web is its support for interoperability.
	Combining knowledge sources from different authors and locations
	yields refined and better results.
	
	
	Current Semantic Web applications only use a limited amount of particularly
	useful and popular information providers like Swoogle, geonames,
	etc. for their queries.
	
	As more and more applications facilitating Semantic Web technologies
	emerge, the load caused by these applications is expected to grow,
	requiring more efficient ways for querying external resources.
	
	
	This research suggests an approach for query optimization based on
	ideas originally proposed by McQueen for optimal stopping in business
	economics.
	
	Applications querying external resources are modeled as decision makers
	looking for optimal action/answer sets, facing search costs for acquiring
	information, test costs for checking the acquired information, and
	receiving a reward depending on the usefulness of the proposed solution.
	
	
	Applying these concepts to the information system domain yields strategies
	for optimizing queries to external services. An extensive evaluation
	compares these strategies to a conventional coverage based approach,
	based on real world response times taken from three different popular
	Web services.},
  isbn = {979-989-8111-51-7},
  owner = {albert},
  timestamp = {2008.05.16}
}

@INCOLLECTION{weichselbraun2009c,
  author = {Albert Weichselbraun},
  title = {Applying Optimal Stopping for Optimizing Queries to External Semantic
	Web Resources},
  booktitle = {Software and Data Technologies},
  publisher = {Springer},
  year = {2009},
  editor = {Cordeiro, J. and Shishkov, B. and Ranchordas, A.K. and Helfert, M.},
  volume = {47},
  series = {Communications in Computer and Information Science},
  pages = {105--118},
  address = {Berlin-Heidelberg},
  abstract = {Semantic Web resources based on ideas originally proposed by MacQueen
	for optimal stopping in business economics. Modeling applications
	as decision makers looking for optimal action/answer sets, facing
	search costs for acquiring information, test costs for checking these
	information, and receiving a reward depending on the usefulness of
	the proposed solution, yields strategies for optimizing queries to
	external services. An extensive evaluation compares these strategies
	to a conventional coverage based approach, based on real world response
	times taken from popular Web services.},
  doi = {10.1007/978-3-642-05201-9},
  eprint = {http://eprints.weblyzard.com/16/1/ccis2009.pdf},
  file = {weichselbraun2009c-optimalStopping.pdf:literature/self/weichselbraun2009c-optimalStopping.pdf:PDF},
  isbn = {978-3-642-05200-2},
  issn = {1865-0929},
  owner = {albert},
  timestamp = {2009.09.20}
}

@UNPUBLISHED{weichselbraun2011t,
  author = {Weichselbraun, Albert},
  title = {Ontology Learning based on Text Mining and Social Evidence Sources},
  note = {University of Western Australia, School of Computer Science and Software
	Engineering, Perth, Australia},
  month = {9 February},
  year = {2011},
  eprint = {http://eprints.weblyzard.com/27/1/uwa2011.pdf},
  owner = {albert}
}

@ARTICLE{weichselbraun2010b,
  author = {Weichselbraun, Albert},
  title = {Optimizing Queries to Remote Resources},
  journal = {Journal of Intelligent Information Systems},
  year = {2011},
  volume = {37},
  number = {2},
  pages = {119-137},
  keywords = {information integration, adaptive decision-making, optimal stopping, opportunity cost model, Semantic Web, heterogeneous data sources},
  abstract = {One key property of the Semantic Web is its support for interoperability.
	Recent research in this area focuses on the integration of multiple
	data sources to facilitate tasks such as ontology learning, user
	query expansion and context recognition. The growing popularity of
	such machups and the rising number of Web APIs supporting links between
	heterogeneous data providers asks for intelligent methods to spare
	remote resources and minimize delays imposed by queries to external
	data sources.
	
	 This paper suggests a cost and utility model for optimizing such
	queries by leveraging optimal stopping theory from business economics:
	applications are modeled as decision makers that look for optimal
	answer sets. Queries to remote resources cause additional cost but
	retrieve valuable information which improves the estimation of the
	answer set's utility. Optimal stopping optimizes the trade-off between
	query cost and answer utility yielding optimal query strategies for
	remote resources. These strategies are compared to conventional approaches
	in an extensive evaluation based on real world response times taken
	from seven popular Web services.},
  doi = {10.1007/s10844-010-0129-0},
  file = {weichselbraun2010b-optimizingQueriesToRemoteResources.pdf:literature/self/weichselbraun2010b-optimizingQueriesToRemoteResources.pdf:PDF},
  issn = {0925-9902},
  eprint = {http://eprints.weblyzard.com/29/1/weichselbraun2011%2DoptimizingQueries.pdf},
  owner = {albert}
}

@UNPUBLISHED{weichselbraun2010t,
  author = {Weichselbraun, Albert},
  title = {Augmenting Lightweight Domain Ontologies with Social Evidence Sources},
  note = {9th International Workshop on Web Semantics, 21th International Conference
	on Database and Expert Systems Application (DEXA 2010), Bilbao, Spain},
  month = {31 August},
  year = {2010},
  eprint = {http://eprints.weblyzard.com/22/1/weichselbraun%2DwebS2010.pdf}
}

@INPROCEEDINGS{weichselbraun2009d,
  author = {Albert Weichselbraun},
  title = {A Utility Centered Approach for Evaluating and Optimizing Geo-Tagging},
  booktitle = {First International Conference on Knowledge Discovery and Information
	Retrieval (KDIR 2009)},
  year = {2009},
  pages = {134--139},
  address = {Madeira, Portugal},
  month = {October},
  abstract = {Geo-tagging is the process of annotating a document with its geographic
	focus by extracting a unique locality that describes the geographic
	context of the document as a whole. Accurate geographic annotations
	are crucial for geospatial applications such as Google Maps or the
	IDIOM Media Watch on Climate Change, but many obstacles complicate
	the evaluation of such tags.
	
	This paper introduces an approach for optimizing geo-tagging by applying
	the concept of utility from economic theory to tagging results. Computing
	utility scores for geo-tags allows a fine grained evaluation of the
	tagger's performance in regard to multiple dimensions specified in
	use case specific domain ontologies and provides means for addressing
	problems such as different scope and coverage of evaluation corpora.
	
	
	The integration of external data sources and evaluation ontologies
	with user profiles ensures that the framework considers use case
	specific requirements. The presented model is instrumental in comparing
	different geo-tagging settings, evaluating the effect of design decisions,
	and customizing geo-tagging to a particular use cases.},
  eprint = {http://eprints.weblyzard.com/13/1/geo.pdf},
  file = {weichselbraun2009d-optimizingGeoTagging.pdf:literature/self/weichselbraun2009d-optimizingGeoTagging.pdf:PDF},
  keywords = {geo-tagging, quality assessment, evaluation, utility model, GeoNames}
}

@UNPUBLISHED{weichselbraun2009t,
  author = {Albert Weichselbraun},
  title = {A Utility Centered Approach for Evaluating and Optimizing Geo-Tagging},
  note = {First International Conference on Knowledge Discovery and Information
	Retrieval (KDIR 2009), Madeira, Portugal},
  month = {7 October},
  year = {2009},
  eprint = {http://eprints.weblyzard.com/13/2/kdir2009.pdf},
  keywords = {geo-tagging, quality assessment, evaluation, utility model, GeoNames},
  url = {http://eprints.weblyzard.com/13/}
}

@UNPUBLISHED{weichselbraun2008t,
  author = {Weichselbraun, Albert},
  title = {Strategies for Optimizing Querying Third Party Resources in Semantic
	Web Applications},
  note = {3rd International Conference on Software and Data Technologies (ICSOFT-2008),
	Porto, Portugal},
  month = {6 July},
  year = {2008},
  eprint = {http://eprints.weblyzard.com/6/1/icsoft2008.pdf},
  file = {:/home/albert/data/ac/conferences/icsoft2008/icsoft2008.pdf:PDF},
  owner = {albert},
  timestamp = {2008.07.15}
}

@UNPUBLISHED{weichselbraun2008ta,
  author = {Weichselbraun, Albert},
  title = {Capturing and Classifying Ontology Evolution in News Media Archives},
  note = {7th International Workshop on Web Semantics, 19th International Workshop
	on Database and Expert Systems Application (DEXA 2008), Turin, Italy},
  month = {2 September},
  year = {2008},
  eprint = {http://eprints.weblyzard.com/2/1/weichselbraun-webS2008.pdf},
  owner = {albert},
  timestamp = {2008.09.01}
}

@UNPUBLISHED{weichselbraun2007t,
  author = {Weichselbraun, Albert},
  title = {Visualisierung von semantischen Topologien mittels dynamischer Wissenslandkarten},
  note = {Association of Austrian market-researchers, Vienna Public Library,
	Vienna, Austria},
  month = {21 February},
  year = {2007},
  file = {:/home/albert/data/ac/talks/pr/wissensmanagement/wissenslandkarten.pdf:PDF},
  owner = {albert},
  timestamp = {2008.07.15}
}

@UNPUBLISHED{weichselbraun2006t,
  author = {Weichselbraun, Albert},
  title = {AVALON - Acquisition and VALidation of Ontologies},
  note = {International Conference on Knowledge Management (iKnow 2006), Invited
	Speaker, Graz, Austria},
  month = {6 September},
  year = {2006},
  file = {:/home/albert/data/ac/conferences/i-know.at/2006/iknow.pdf:PDF},
  owner = {albert},
  timestamp = {2008.07.15}
}

@PHDTHESIS{weichselbraun2004,
  author = {Weichselbraun, Albert},
  title = {Ontologiebasierende Textklassifikation mittels mathematischer Verfahren},
  school = {Vienna University of Economics and Business Administration},
  year = {2004}
}

@UNPUBLISHED{weichselbraun2004t,
  author = {Weichselbraun, Albert},
  title = {Dynamic, ontology based text classification},
  note = {Curtin Centre for Extended Enterprises and Business Intelligence,
	Curtin University, Perth, Australia},
  month = {3 December},
  year = {2004},
  owner = {albert},
  timestamp = {2008.07.15}
}

@UNPUBLISHED{weichselbraun2004ta,
  author = {Weichselbraun, Albert},
  title = {Ontology based Text Classification Using Mathematical Methods},
  note = {medienKUNSTLABOR, Kunsthaus Graz, Austria},
  month = {23 April},
  year = {2004},
  owner = {albert},
  timestamp = {2008.07.15}
}

@MASTERSTHESIS{weichselbraun2002,
  author = {Weichselbraun, Albert},
  title = {Metadaten im Web},
  school = {Vienna University of Economics and Business Administration},
  year = {2002},
  address = {Vienna, Austria},
  owner = {albert},
  timestamp = {2008.07.14},
  url = {http://www.ai.wu.ac.at/~albert/}
}

@MASTERSTHESIS{weichselbraun2001,
  author = {Weichselbraun, Albert},
  title = {Vergleich unterschiedlicher Finite-Volumen-Verfahren zur numerischen
	Simulation der Strömung in einem beheizten Rohrnetzwerk},
  school = {Vienna University of Technology},
  year = {2001},
  owner = {albert},
  timestamp = {2008.07.14}
}

@ARTICLE{weichselbraun2010e,
  author = {Weichselbraun, Albert and Gindl, Stefan and Scharl, Arno},
  title = {A Context-Dependent Supervised Learning Approach to Sentiment Detection
	in Large Textual Databases},
  journal = {Journal of Information and Data Management},
  year = {2010},
  volume = {1},
  pages = {329--342},
  number = {3},
  abstract = {Sentiment detection automatically identifies emotions in textual data.
	The increasing amount of emotive documents available in corporate
	databases and on the World Wide Web calls for automated methods to
	process this important source of knowledge. Sentiment detection draws
	attention from researchers and practitioners alike - to enrich business
	intelligence applications, for example, or to measure the impact
	of customer reviews on purchasing decisions. Most sentiment detection
	approaches do not consider language ambiguity, despite the fact that
	one and the same sentiment term might differ in polarity depending
	on the context, in which a statement is made. To address this shortcoming,
	this paper introduces a novel method that uses Naïve Bayes to identify
	ambiguous terms. A contextualized sentiment lexicon stores the polarity
	of these terms, together with a set of co-occurring context terms.
	A formal evaluation of the assigned polarities confirms that considering
	the usage context of ambiguous terms improves the accuracy of high-throughput
	sentiment detection methods. Such methods are a prerequisite for
	using sentiment as a metadata element in storage and distributed
	file-level intelligence applications, as well as in enterprise portals
	that provide a semantic repository of an organization's information
	assets.},
  eprint = {http://eprints.weblyzard.com/25/1/jidm%2Dpublished%2D1.pdf}
}

@INPROCEEDINGS{weichselbraun2008a,
  author = {Weichselbraun, Albert and Scharl, Arno and Liu, Wei},
  title = {Capturing and Classifying Ontology Evolution in News Media Archives},
  booktitle = {7th International Workshop on Web Semantics, 19th International Workshop
	on Database and Expert Systems Application (DEXA 2008)},
  year = {2008},
  editor = {Tjoa, A. Min and Wagner, Roland R.},
  pages = {197-201},
  address = {Turin, Italy},
  month = {September},
  publisher = {IEEE Computer Society Press},
  abstract = {Ontology evolution is an intrinsic phenomenon of any knowledge-intensive
	system, which can be addressed either implicitly or explicitly.
	
	This paper describes an approach to capture and visualize implicit
	data-driven ontology evolution using ontologies semi-automatically
	generated by extending small seed ontologies.
	
	This process captures ontology changes reflected in large document
	collections. Visualizing of these changes helps characterize the
	
	evolution process, and distinguish core, extended and peripheral relations
	between concepts. Finally, the paper presents an example of ontology
	evolution by monitoring and analyzing online media coverage on ``energy
	sources'' over a period of ten months.},
  isbn = {978-0-7695-3299-8},
  owner = {albert},
  timestamp = {2008.09.01}
}

@INPROCEEDINGS{weichselbraun2007b,
  author = {Weichselbraun, Albert and Scharl, Arno and Liu, Wei and Wohlgenannt,
	Gerhard},
  title = {Capturing Ontology Evolution Processes by Repeated Sampling of Large
	Document Collections},
  booktitle = {On the Move to Meaningful Internet Systems 2007: CoopIS, DOA, ODBASE,
	GADA, and IS · OTM Confederated International Conferences, CoopIS,
	DOA, ODBASE, GADA, and IS 2007},
  year = {2007},
  editor = {Meersman, R. and Tari, Z. and Herrero, P.},
  volume = {4803},
  series = {Lecture Notes in Computer Science},
  address = {Vilamoura, Portugal},
  month = {November},
  owner = {albert},
  pages = {23--24},
  timestamp = {2007.11.19}
}

@INPROCEEDINGS{weichselbraun2010a,
  author = {Weichselbraun, Albert and Wohlgenannt, Gerhard and Scharl, Arno},
  title = {Augmenting Lightweight Domain Ontologies with Social Evidence Sources},
  booktitle = {9th International Workshop on Web Semantics, 21st International Conference
	on Database and Expert Systems Applications (DEXA 2010)},
  year = {2010},
  editor = {Tjoa, A. Min and Wagner, Roland R.},
  pages = {193--197},
  address = {Bilbao, Spain},
  month = {August},
  publisher = {IEEE Computer Society Press},
  abstract = {Recent research shows the potential of utilizing data collected through
	Web 2.0 applications to capture changes in a domain's terminology.
	This paper presents an approach to augment corpus-based ontology
	learning by considering terms from collaborative tagging systems,
	social networking platforms, and micro-blogging services. The proposed
	framework collects information on the domain's terminology from domain
	documents and a seed ontology in a triple store. Data from social
	sources such as Delicious, Flickr, Technorati and Twitter provide
	an outside view of the domain and help incorporate external knowledge
	into the ontology learning process. The neural network technique
	of spreading activation is used to identify relevant new concepts,
	and to determine their positions in the extended ontology. Evaluating
	the method with two measures (PMI and expert judgements) demonstrates
	the significant benefits of social evidence sources for ontology
	learning. },
  owner = {albert},
  timestamp = {2010.05.07}
}

@INCOLLECTION{weichselbraun2010c,
  author = {Albert Weichselbraun and Gerhard Wohlgenannt and Arno Scharl},
  title = {Evidence Sources, Methods and Use Cases for Learning Lightweight
	Domain Ontologies},
  booktitle = {Ontology Learning and Knowledge Discovery Using the Web: Challenges
	and Recent Advances},
  publisher = {IGI Global},
  pages = {1--15},
  year = {2010},
  editor = {Wilson Wong and Wei Liu and Mohammed Bennamoun},
}

@INPROCEEDINGS{weichselbraun2011,
  author = {Weichselbraun, Albert and Wohlgenannt, Gerhard and Scharl, Arno},
  title = {Applying Optimal Stopping Theory to Improve the Performance of Ontology
	Refinement Methods},
  booktitle = {Proceedings of the 44th Hawaii International Conference on System
	Sciences (HICSS-44)},
  year = {2011},
  pages = {1--10},
  isbn = {978-0-7695-4282-9},
  address = {Maui, Hawaii},
  month = {January},
  publisher = {IEEE Computer Society Press},
  abstract = { Recent research shows the potential of utilizing data collected through
	Web 2.0 applications to capture domain evolution. Relying on external
	data sources, however, often introduces delays due to the time spent
	retrieving data from these sources. The method introduced in this
	paper streamlines the data acquisition process by applying optimal
	stopping theory. An extensive evaluation demonstrates how such an
	optimization improves the processing speed of an ontology refinement
	component which uses Delicious to refine ontologies constructed from
	unstructured textual data while having no significant impact on the
	quality of the refinement process. Domain experts compare the results
	retrieved from optimal stopping with data obtained from standardized
	techniques to assess the effect of optimal stopping on data quality
	and the created domain ontology.},
  owner = {albert},
  eprint = {http://eprints.weblyzard.com/31/1/weichselbraun2011%2DperformanceOfOntologyRefinement.pdf},
  timestamp = {2007.10.31}
}

@INPROCEEDINGS{weichselbraun2011a,
  author = {Albert Weichselbraun and Stefan Gindl and Arno Scharl},
  title = {Using Games with a Purpose and Bootstrapping to Create Domain-Specific Sentiment Lexicons},
  booktitle = {Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2011)},
  year = {2011},
  address = {Glasgow, Scotland},
  numpages = {8},
  pages = {1053--1060},
  isbn = {978-1-4503-0717-8},
  doi = {10.1145/2063576.2063729},
  month = {October},
  publisher = {ACM},
  owner = {albert},
  eprint = {http://eprints.weblyzard.com/33/1/weichselbraun2011-gamesWithAPurpose.pdf},
  keywords = {Sentiment Detection, Sentiment Analysis, Bootstrapping, Language Resources, Sentiment Lexicon, Crowd-Sourcing},
  abstract = {Sentiment detection analyzes the positive or negative polarity of text. The field has received considerable attention in recent years, since it plays an important role in providing means to assess user opinions regarding an organisation's products, services, or actions.

  Approaches towards sentiment detection include machine learning techniques as well as computationally less expensive methods. The latter rely on the use of language-specific sentiment lexicons, which are lists of sentiment terms with their corresponding sentiment value. The effort involved in creating, customizing, and extending sentiment lexicons is considerable, particularly if less common languages and domains are targeted without access to appropriate language resources.

  This paper proposes a semi-automatic approach for the creation of sentiment lexicons which assigns sentiment values to sentiment terms via crowdsourcing. Furthermore, it introduces a bootstrapping process operating on unlabeled domain documents to extend the created lexicons, and to customize them according to the particular use case. This process considers sentiment terms as well as sentiment indicators occurring in the discourse surrounding a particular topic. Such indicators are associated with a positive or negative context in a particular domain, but might have a neutral connotation in other domains.

  A formal evaluation shows that bootstrapping considerably improves the method's recall. Automatically created lexicons yield a performance comparable to professionally created language resources such as the General Inquirer.}
}


@ARTICLE{weichselbraun2010,
  author = {Weichselbraun, Albert and Wohlgenannt, Gerhard and Scharl, Arno},
  title = {Refining Non-Taxonomic Relation Labels with External Structured Data
	to Support Ontology Learning},
  journal = {Data \& Knowledge Engineering},
  year = {2010},
  volume = {69},
  pages = {763--778},
  number = {8},
  abstract = {This paper presents a method to integrate external knowledge sources
	such as DBpedia and OpenCyc into an ontology learning system that
	automatically suggests labels for unknown relations in domain ontologies
	based on large corpora of unstructured text. The method extracts
	and aggregates verb vectors from semantic relations identified in
	the corpus. It composes a knowledge base which consists of (i) verb
	centroids for known relations between domain concepts, (ii) mappings
	between concept pairs and the types of known relations, and (iii)
	ontological knowledge retrieved from external sources. Applying semantic
	inference and validation to this knowledge base yields a refined
	relation label suggestion. A formal evaluation compares the accuracy
	and average ranking precision of this hybrid method with the performance
	of methods that solely rely on corpus data and those that are only
	based on reasoning and external data sources. },
  keywords = {Ontologies, Semantic Web, Ontology Learning, Relation Labeling, Machine Learning},
  doi = {10.1016/j.datak.2010.02.010},
  eprint = {http://eprints.weblyzard.com/18/1/weichselbraun2010.pdf},
  file = {weichselbraun2010-refiningNonTaxonomicRelationLabels.pdf:literature/self/weichselbraun2010-refiningNonTaxonomicRelationLabels.pdf:PDF},
  issn = {0169-023X},
  owner = {albert}
}

@INPROCEEDINGS{weichselbraun2007,
  author = {Weichselbraun, Albert and Wohlgenannt, Gerhard and Scharl, Arno and
	Granitzer, Michael and Neidhard, Thomas and Juffinger, Andreas},
  title = {Applying Vector Space Models to Ontology Link Type Suggestion},
  booktitle = {4th International Conference on Innovations in Information Technology},
  year = {2007},
  pages = {566-570},
  address = {Dubai, United Arab Emirates},
  month = {November},
  doi = {978-1-4244-1841-1},
  owner = {albert},
  timestamp = {2007.11.19}
}

@ARTICLE{weichselbraun2009,
  author = {Weichselbraun, Albert and Wohlgenannt, Gerhard and Scharl, Arno and
	Granitzer, Michael and Neidhart, Thomas and Juffinger, Andreas},
  title = {Discovery and Evaluation of Non-Taxonomic Relations in Domain Ontologies},
  journal = {International Journal of Metadata, Semantics and Ontologies},
  year = {2009},
  volume = {4},
  pages = {212--222},
  number = {3},
  abstract = { The identification and labelling of non-hierarchical relations are
	among the most challenging tasks in ontology learning. This paper
	describes a bottom-up approach for automatically suggesting ontology
	link types. The presented method extracts verb-vectors from semantic
	relations identified in the domain corpus, aggregates them by computing
	centroids for known relation types, and stores the centroids in a
	central knowledge base. Comparing verb-vectors extracted from unknown
	relations with the stored centroids yields link type suggestions.
	Domain experts evaluate these suggestions, refining the knowledge
	base and constantly improving the component's accuracy. A final evaluation
	provides a detailed statistical analysis of the introduced approach.},
  biburl = {http://www.inderscience.com/search/index.php?action=record\&rec\_id=27755},
  eprint = {http://eprints.weblyzard.com/11/1/ontology\_linktype.pdf},
  file = {weichselbraun2009-nonTaxonomicRelations.pdf:literature/self/weichselbraun2009-nonTaxonomicRelations.pdf:PDF},
  owner = {albert},
  timestamp = {2009.03.02}
}

@INPROCEEDINGS{weichselbraun2009a,
  author = {Wohlgenannt, Gerhard and Weichselbraun, Albert and Scharl, Arno},
  title = {Integrating Structural Data into Methods for Labeling Relations in
	Domain Ontologies},
  booktitle = {8th International Workshop on Web Semantics, 20th International Workshop
	on Database and Expert Systems Application (DEXA 2009)},
  year = {2009},
  editor = {Tjoa, A. Min and Wagner, Roland R.},
  pages = {94--98},
  address = {Linz, Austria},
  month = {September},
  publisher = {IEEE Computer Society Press},
  abstract = {This paper presents a method for integrating DBpedia data into an
	ontology learning system that automatically suggests labels for relations
	in domain ontologies based on large corpora of unstructured text.
	The method extracts and aggregates verb vectors for semantic relations
	identified in the corpus. It composes a knowledge base which consists
	of (i) centroids for known relations between domain concepts, (ii)
	mappings between concept pairs and the types of known relations,
	and (iii) ontological knowledge retrieved from DBpedia. Refining
	similarities between the verb centroids of labeled and unlabeled
	relations by means of including domain and range constraints applying
	DBpedia data yields relation type suggestions. A formal evaluation
	compares the accuracy and average ranking performance of this hybrid
	method with the performance of methods that solely rely on corpus
	data and those that are only based on reasoning and external data
	sources.},
  doi = {10.1007/978-3-642-05201-9_9}
}

@INPROCEEDINGS{scharl2011,
  author = "Scharl, Arno and Hubmann-Haidvogel, Alexander and Wohlgenannt, Gerhard and Weichselbraun, Albert and Dickinger, Astrid",
  year   = "2011",
  title  = "Scalable Annotation Mechanisms for Digital Content Aggregation and Context-Aware Authoring",
  booktitle = "Proceedings of the 10th Brazilian Symposium on Human Factors in Computer Systems \& 5th Latin American Conference on
               Human-Computer Interaction (IHC-CLIHC-2011)",
  address = "Porto de Galinhas, Brazil",
  publisher = "Brazilian Computing Society",
  pages     = "376-380",
  keywords  = "Content production and consumption, context-awareness,
  classification, real-time annotation, collaborative authoring",
  abstract  = "This paper discusses the role of context information in
        building the next generation of human-centered information
        systems, and classifies the various aspects of contextualization 
        with a special emphasis on the production and consumption of digital content. 
        The real-time annotation of
        resources is a crucial element when moving from content
        aggregators (which process third-party digital content) to
        context-aware visual authoring environments (which allow
        users to create and edit their own documents). We present a
        publicly available prototype of such an environment, which
        required a major redesign of an existing Web intelligence
        and media monitoring framework to provide real-time data
        services and synchronize the text editor with the frontend’s
        visual components. The paper concludes with a summary of
        achieved results and an outlook on possible future research
        avenues including multi-user support and the visualization
        of document evolution.",
 eprint = "http://eprints.weblyzard.com/32/1/scharl2011%2DscalableAnnotationMechanisms.pdf",
}



@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

